# 阅读笔记

[TOC]

## 3D人脸动画

## Capture, Learning, and Synthesis of 3D Speaking Styles

### 摘要

本文提出了一种与说话声音以及人脸形状无关的语音表情驱动方法。其中音频通过现有的**DeepSpeech**提取**MFCC**音频特征，通过编码器提取音频特征，并在特征上叠加区分说话人(及风格)的编码，用于提取说话人风格以及音频特征。通过解码器输出逐顶点的偏移量，叠加到人脸模版上，即可得到语音对应的表情动画。根据合成的表情模型采用现有的**FLAME**模型很容易提取到当前的人脸形状和表情系数，通过修改或替换人脸形状系数，即可得到任意脸型的说话表情。

### 模型架构(VOCA)

![image-20211002095231544](E:\论文\阅读笔记.assets\image-20211002095231544.png)

![image-20211002095157881](E:\论文\阅读笔记.assets\image-20211002095157881.png)

### 数据集(VOCASET)

3D头部mesh+音频



**Encoder**：

```python
输入: 
Encoder网络定义:
audio_encoder = SpeechEncoder(self.config)

Encoder数据定义:
condition = tf.one_hot(indices=self.condition_subject_id, depth=self.batcher.get_num_training_subjects())
audio_encoder(self.speech_features, condition, self.is_training)
```

**Decoder**

```python
Decoder网络定义:
expression_decoder = ExpressionLayer(self.config)

Decoder数据定义:
# 顶点偏移
self.expression_offset = expression_decoder(self.output_encoder)
# input_template+offset
self.output_decoder = tf.add(self.expression_offset, self.input_template, name='output_decoder')
```





**FLAME对齐后，每个网格由5023个3D顶点组成**

**face_vert_mmap**

![image-20211002105802737](E:\论文\阅读笔记.assets\image-20211002105802737.png)



![image-20211002105815197](E:\论文\阅读笔记.assets\image-20211002105815197.png)



**我是谁？** **num_consecutive_frames**：window_size

**还有我？** **compute_window_array_idx**

**templates_data**



**processed_audio**：(sub-12, sen-40, keys('audio', 'sample_rate'),  (frames, 16, 29))





**data2array_verts**：

​	**dict**：{subject_name：

​					{sentence_name：

​						{frame：array_idx}

​																	}

array2data_verts

**array2data[array_idx] = (sub, seq, frame)**



**array2window_ids[array_idx] = [data2array[sub][seq][id] for id in window_frames]**

```python
for frame, array_idx in data2array[sub][seq].items():

`**array2data[array_idx] = (sub, seq, frame)**

**array2window_ids**：array2window_ids[array_idx] = [data2array[sub][seq][id] for id in window_frames]
```



### 训练过程

#### loss

```python
self.rec_loss = self._reconstruction_loss()
self.velocity_loss = self._velocity_loss()
self.acceleration_loss = self._acceleration_loss()
self.verts_reg_loss = self._verts_regularizer_loss()

self.loss = self.rec_loss + self.velocity_loss + self.acceleration_loss + self.verts_reg_loss

self.t_vars = tf.trainable_variables()
```

#### global_step/global_learning_rate/optim

```python
self.global_step = tf.Variable(0, name='global_step', trainable=False)

decay_steps = self.batcher.get_training_size()//self.config['batch_size']
decay_rate = self.config['decay_rate']
if decay_rate < 1:
	self.global_learning_rate = tf.train.exponential_decay(self.config['learning_rate'], 	self.global_step,
	decay_steps, decay_rate, staircase=True)
else:
	self.global_learning_rate = tf.constant(self.config['learning_rate'], dtype=tf.float32)
tf.summary.scalar('learning_rate_training', self.global_learning_rate, collections=['train'])
tf.summary.scalar('learning_rate_validation', self.global_learning_rate, collections=['validation'])

# Adam优化器
self.optim = tf.train.AdamOptimizer(self.global_learning_rate, self.config['adam_beta1_value']).minimize(self.loss, var_list=self.t_vars, global_step=self.global_step)
self._init_summaries()
tf.global_variables_initializer().run()
```

#### summaries

```python
def _init_summaries(self):
	self.train_summary = tf.summary.merge_all('train')
	self.validation_summary = tf.summary.merge_all('validation')
	self.train_writer = tf.summary.FileWriter(os.path.join(self.config['checkpoint_dir'], 'summaries', 'train'))
	self.validation_writer = tf.summary.FileWriter(os.path.join(self.config['checkpoint_dir'], 'summaries', 'validation'))
```

#### _training_step

```python
processed_audio, vertices, templates, subject_idx = self.batcher.get_training_batch(self.config['batch_size'])
feed_dict = {self.speech_features: np.expand_dims(processed_audio, -1),
             self.condition_subject_id: np.array(subject_idx),
             self.is_training: True,
             self.input_template: np.expand_dims(templates, -1),
             self.target_vertices: np.expand_dims(vertices, -1)}

loss, g_step, summary, g_lr, _ = self.session.run([self.loss, self.global_step, self.train_summary, 														self.global_learning_rate, self.optim], feed_dict)
return loss, g_step, summary, g_lr



    def get_training_batch(self, batch_size):
        """
        Get batch for training
        :param batch_size:
        :return:
        """
        if self.current_state == 0:
            random.shuffle(self.training_indices)

        if (self.current_state + batch_size) > (len(self.training_indices) + 1):
            self.current_state = 0
            return self.get_training_batch(batch_size)
        else:
            self.current_state += batch_size
            batch_indices = self.training_indices[self.current_state:(self.current_state + batch_size)]
            if len(batch_indices) != batch_size:
                self.current_state = 0
                return self.get_training_batch(batch_size)
            return self.data_handler.slice_data(batch_indices)
        
    def _slice_data(self, indices):
        if self.num_consecutive_frames == 1:
            return self._slice_data_helper(indices)
        else:
            window_indices = []
            for id in indices:
                window_indices += self.array2window_ids[id]
            return self._slice_data_helper(window_indices)

    def _slice_data_helper(self, indices):
        face_vertices = self.face_vert_mmap[indices]
        face_templates = []
        processed_audio = []
        subject_idx = []
        for idx in indices:
            sub, sen, frame = self.array2data_verts[idx]
            face_templates.append(self.templates_data[sub])
            if self.processed_audio is not None:
                processed_audio.append(self.processed_audio[sub][sen]['audio'][frame])
            subject_idx.append(self.convert_training_subj2idx(sub))

        face_templates = np.stack(face_templates)
        subject_idx = np.hstack(subject_idx)
        assert face_vertices.shape[0] == face_templates.shape[0]

        if self.processed_audio is not None:
            processed_audio = np.stack(processed_audio)
            assert face_vertices.shape[0] == processed_audio.shape[0]
        return processed_audio, face_vertices, face_templates, subject_idx
```

##### reconstruction_loss

```python
rec_loss = reconstruction_loss(predicted=self.output_decoder, real=self.target_vertices,
                                                           want_absolute_loss=self.config[
                                                               'absolute_reconstruction_loss'])
                                                               
def reconstruction_loss(predicted, real, want_absolute_loss=True, want_in_mm=False, weights=None):
    if weights is not None:
        assert predicted.shape[1] == real.shape[1] == weights.shape[0]
        tf_weights = tf.constant(weights, dtype=tf.float32)
        predicted = tf.einsum('abcd,bd->abcd', predicted, tf_weights)
        real = tf.einsum('abcd,bd->abcd', real, tf_weights)

    if want_in_mm:
        predicted, real = predicted * 1000, real * 1000
    if want_absolute_loss:
        return tf.reduce_mean(tf.reduce_sum(tf.abs(tf.subtract(predicted, real)), axis=2))
    else:
        return tf.reduce_mean(tf.reduce_sum(tf.squared_difference(predicted, real), axis=2))
```

##### velocity_loss

```python
if self.config['velocity_weight'] > 0.0:
            assert(self.config['num_consecutive_frames'] >= 2)
            verts_predicted = tf.reshape(self.output_decoder, [-1, self.config['num_consecutive_frames'], self.config['num_vertices'], 3])
            x1_pred = tf.reshape(verts_predicted[:, -1, :], [-1, self.config['num_vertices'], 3, 1])
            x2_pred = tf.reshape(verts_predicted[:, -2, :], [-1, self.config['num_vertices'], 3, 1])
            velocity_pred = x1_pred-x2_pred

            verts_target = tf.reshape(self.target_vertices, [-1, self.config['num_consecutive_frames'], self.config['num_vertices'], 3])
            x1_target = tf.reshape(verts_target[:, -1, :], [-1, self.config['num_vertices'], 3, 1])
            x2_target = tf.reshape(verts_target[:, -2, :], [-1, self.config['num_vertices'], 3, 1])
            velocity_target = x1_target-x2_target

            with tf.name_scope('Velocity_loss'):
                velocity_loss = self.config['velocity_weight']*reconstruction_loss(predicted=velocity_pred, real=velocity_target,
                                                        want_absolute_loss=self.config['absolute_reconstruction_loss'])
            tf.summary.scalar('velocity_loss_training', velocity_loss, collections=['train'])
            tf.summary.scalar('velocity_loss_validation', velocity_loss, collections=['validation'])
            return velocity_loss
        else:
            return 0.0
```

##### acceleration_loss

```python
if self.config['acceleration_weight'] > 0.0:
            assert(self.config['num_consecutive_frames'] >= 3)
            verts_predicted = tf.reshape(self.output_decoder, [-1, self.config['num_consecutive_frames'], self.config['num_vertices'], 3])
            x1_pred = tf.reshape(verts_predicted[:, -1, :], [-1, self.config['num_vertices'], 3, 1])
            x2_pred = tf.reshape(verts_predicted[:, -2, :], [-1, self.config['num_vertices'], 3, 1])
            x3_pred = tf.reshape(verts_predicted[:, -3, :], [-1, self.config['num_vertices'], 3, 1])
            acc_pred = x1_pred-2*x2_pred+x3_pred

            verts_target = tf.reshape(self.target_vertices, [-1, self.config['num_consecutive_frames'], self.config['num_vertices'], 3])
            x1_target = tf.reshape(verts_target[:, -1, :], [-1, self.config['num_vertices'], 3, 1])
            x2_target = tf.reshape(verts_target[:, -2, :], [-1, self.config['num_vertices'], 3, 1])
            x3_target = tf.reshape(verts_target[:, -3, :], [-1, self.config['num_vertices'], 3, 1])
            acc_target = x1_target-2*x2_target+x3_target

            with tf.name_scope('Acceleration_loss'):
                acceleration_loss = self.config['acceleration_weight']*reconstruction_loss(predicted=acc_pred, real=acc_target,
                                                        want_absolute_loss=self.config['absolute_reconstruction_loss'])
            tf.summary.scalar('acceleration_loss_training', acceleration_loss, collections=['train'])
            tf.summary.scalar('acceleration_loss_validation', acceleration_loss, collections=['validation'])
            return acceleration_loss
        else:
            return 0.0
```

##### verts_regularizer_loss

```python
if self.config['verts_regularizer_weight'] > 0.0:
            with tf.name_scope('Verts_regularizer_loss'):
                verts_regularizer_loss = self.config['verts_regularizer_weight']*tf.reduce_mean(tf.reduce_sum(tf.abs(self.expression_offset), axis=2))
            tf.summary.scalar('verts_regularizer_losss_training', verts_regularizer_loss, collections=['train'])
            tf.summary.scalar('verts_regularizer_loss_validation', verts_regularizer_loss, collections=['validation'])
            return verts_regularizer_loss
        else:
            return 0.0
```



### DeepSpeech

