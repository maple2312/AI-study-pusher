### 统计学习基本知识



#### 损失函数和风险函数

##### 损失函数

给定一个输入 $X$ 和决策函数 $f$ ，$X$ 对于的真实值为 $Y$ ，而有决策函数得到的预测值为 $f(X)$，则可以定义一个损失函数 $L(Y, f(X))$ 来描述真实值和预测值之间的差异。

以下为几种常见的损失

- 0 - 1 损失
  $$
  L(Y, f(X)) = \begin{cases}1 & Y \ne f(X)\\ 0 & Y = f(X)\end{cases}
  $$

- 平方损失
  $$
  L(Y, f(X)) = (Y - f(X))^2
  $$

- 对数损失（对数似然损失）
  $$
  L(Y, P(Y|X)) = -\mathrm{log}{P(Y|X)}
  $$
  

等等……



##### 风险函数（期望损失）

损失函数 $L(Y, f(X))$ 的期望风险`expected risk`为
$$
\begin{align}
R_{exp} (f) &= E_p[L(Y, f(X))] \\\\
		&= \int_{\mathcal{X} \times \mathcal{Y}} L(y, f(x)) \times P(x, y)\ dxdy
\end{align}
$$
给定一个训练数据集
$$
T = \{ (x_1, y_1), (x_2, y_2), ..., (x_N, y_N) \}
$$
模型 $f(X)$ 关于该训练集的平均损失称为经验风险`empirical risk`，为
$$
R_{emp}(f) = {\frac{1}{N}}\sum_{i=1}^{N}L(y_i, f(x_i))
$$


> 期望风险 $R_{exp} (f)$ 为模型关于联合分布的期望损失，经验风险 $R_{emp}(f)$ 是模型关于训练样本集的平均损失。



#### 弱大数定律

假设随机变量$X_1, X_2, ... , X_n, ....$ 是独立同分布的，且有全体期望 $E(X_k) = \mu$，取前 $n$ 个序列，当 $n \rightarrow \infty$ 时有
$$
\forall \epsilon \gt 0 \quad p\{ (\mu - \frac{\sum_{i=1}^{n}X_i}{n}) \lt \epsilon \} = 1
$$

根据大数定律当 $N \rightarrow \infty$ 时，有经验风险趋向于期望风险
$$
R_{emp}(f) \rightarrow R_{exp}(f)
$$



#### 正则化

正则化有如下的一般形式
$$
\min_{f \in \mathcal{F}} {\frac{1}{N}}\sum_{i = 1}^{N}L(y_i, f(x_i)) + \lambda J(f) \quad \lambda > 0
$$
其中 $\lambda J(f)$ 为正则化项，$J(f)$ 与模型的复杂度呈正相关，$\lambda$ 为调整经验风险与模型复杂度之间关系的系数

- $L_1$ 范数为 $\lambda ||w||_1$ （各参数绝对值之和），其倾向于将各参数调整为 $0$，故可产生稀疏权值矩阵，一般用于特征选择
- $L_2$ 范数为 $\lambda ||w||_2$，（各参数求平方和后求平方根），因为要进行开方操作，故其主要影响绝对值大的参数，一般用于防止模型过拟合

正则化的作用就是选择经验风险和模型复杂度同时较小的模型。



#### 泛化能力

泛化能力指的是模型对未知数据的预测能力。

泛化误差定义如下
$$
\begin{align}
R_{exp} (\hat{f}) &= E_p[L(Y, \hat{f}(X))] \\\\
		&= \int_{\mathcal{X} \times \mathcal{Y}} L(y, \hat{f}(x)) \times P(x, y)\ dxdy
\end{align}
$$
其中$\hat{f}(X)$表示上面的 **^** 当前的模型是通过学习得到的。



##### 泛化误差上界

对于二分类问题，当假设空间是有限个函数的集合 $\mathcal{F} = \{f_1, f_2, ..., f_d\}$ 时，对于 $\forall f \in \mathcal{F}$，至少以概率 $1 - \delta, \quad 0 < \delta < 1$  ，以下不等式成立
$$
\begin{align}
& R_{exp}(f) \leq \hat{R}_{emp}(f) + \epsilon(d, N, \delta) \\\\
& \epsilon(d, N, \delta) = \sqrt{\frac{1}{2N}(logd + log\frac{1}{\delta})}
\end{align}
$$
证：

先给出 `Hoeffding` 不等式                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  的定义

设独立同分布的随机变量$X_1, X_2, X_3, ... ,X_N$，其中$X_i \in [a_i, b_i], \quad i = 1, 2, ..., N$  ，记$\overline{X} = \frac{1}{N}\sum_{i=1}^{N}X_i$ ，对于 $\forall \epsilon > 0 $ 有
$$
P(\overline{X} - E(\overline{X}) \geq \epsilon) \leq exp({- \frac {2N^2\epsilon ^ 2}{\sum_{i=1}^{N}(b_i - a_i)^2}}) \\
P(E(\overline{X} - \overline{X}) \geq \epsilon) \leq exp({- \frac {2N^2\epsilon ^ 2}{\sum_{i=1}^{N}(b_i - a_i)^2}})
$$
故有
$$
\begin{align}
& P(R_{exp}(f) - \hat{R}_{emp}(f) \geq \epsilon ) \leq exp(-2N\epsilon^2) \\\\
& P(\exist f \in \mathcal{F}: R_{exp}(f) - \hat{R}_{emp}(f) \geq \epsilon) \leq \sum_{f \in \mathcal{F}} P(R_{exp}(f) - \hat{R}_{emp}(f) \geq \epsilon) \leq d \times exp(-2N\epsilon^2)
\end{align}
$$
$\iff$
$$
\begin{align}
& \forall f \in \mathcal{F} \\\\
& P(R_{exp}(f) - \hat{R}_{emp}(f) \leq \epsilon) \geq 1 -  d \times exp(-2N\epsilon^2) \\\\
& 令 \quad \delta = d \times exp(-2N\epsilon^2) \Rightarrow \epsilon = \sqrt{\frac{1}{2N}(logd + log\frac{1}{\delta})} \\\\
& P(R_{exp}(f) - \hat{R}_{emp}(f) \leq \epsilon) \geq 1 -  \delta \\\\
\end{align}
$$
$\Rightarrow $

至少有 $1-\delta$ 的概率使得
$$
R_{exp}(f) - \hat{R}_{emp}(f) \leq \epsilon
$$
即
$$
R_{exp}(f)  \leq \hat{R}_{emp}(f) + \epsilon
$$
其中
$$
\epsilon = \sqrt{\frac{1}{2N}(logd + log\frac{1}{\delta})}
$$
证毕#



#### 习题

一、说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型n次独立的数据生成结果，其中k次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。

答：

统计学习方法的三要素分别是：

1. 模型：即为伯努利模型，其是定义在取值为0、1的随机变量上的概率分别。对于随机变量 $X$ ，有 $P(X=1) = p, P(X=0) = 1-p \quad 0\leq p\leq 1$ 
2. 策略：对数损失函数
3. 算法
   - 极大似然估计：经验风险最小化
   - 贝叶斯估计：求取参数的后验分布，然后取其期望



设有独立的随机变量 $X_1, X_2, ..., X_N$ 服从 $0-1$ 分布，对于随机变量 $X$，有 $P(X=1) = p, P(X=0) = 1-p$ ，且其中有$k$个样本取$1$，

- 极大似然估计

  概率 $p$ 为待估计的参数，设似然函数 $L(p) = \prod_{i=0}^N{P(x_i)}$，有
  $$
  \begin{align}
  \mathrm{ln}(L(p)) & = \sum_{i=0}^{N}ln(P(X_i)) \\\\
  		 & = kln(p) + (1-k)ln(n-k)
  \end{align}
  $$

  $$
  \begin{align}
  {\mathrm{d} \mathrm{ln}(L(p)) \over \mathrm{d} p} = {k \over p } - {1-k \over n-k} \\\\
  \end{align}
  $$

  令上式为$0$，得到$p$的估计值 $\hat p$
  $$
  \hat p = {k \over n}
  $$

- 贝叶斯估计

  将 $p$ 看作是随机变量，假设其先验分布服从$0-1$的均匀分布，则有概率概率分布函数 $F(p)$ 和概率密度函数 $f(p)$ 分别为
  $$
  F(p) = p \\
  f(p) = 1 \\
  0 \leq p \leq 1
  $$
  有 $p$ 的后验概率分布
  $$
  \begin{align}
  f(p|X_1, X_2, ..., X_N) & = {f(X_1, X_2, ..., X_N | p) \times f(p) \over f(X_1, X_2, ..., X_N)} \\\\
  & = C \times f(X_1, X_2, ..., X_N | p)
  \end{align}
  $$
  其中$f(X_1, X_2, ..., X_N | p)$为在概率$p$的情况下的随机变量的先验概率
  $$
  f(X_1, X_2, ..., X_N | p) = \prod_{i=1}^{N}f(X_i) = p^k\times(1-p)^{n-k}
  $$
  上式对$p$求导后得到到$p$的估计值 $\hat p$为
  $$
  \hat p = {k \over n}
  $$
  此时$p$的后验概率最大。

 



二、证明当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。

设有 $i.i.d$ 的随机变量 $x_1, x_2, ..., x_N$ ，模型（条件概率分布）为 $f \in \mathcal{F}$，模型的参数设置为 $\theta$，损失函数定义为 $L(Y, f(Y|X))$ 

- 经验风险最小化公式为
  $$
  \min_{f \in \mathcal{F}}arg {1 \over N} {\sum_{i=1}^{N} L(y_i, f(y_i|x_i;\theta))}
  $$

- 极大似然估计

  由于样本满足 $i.i.d$，则有
  $$
  \begin{align}
  & L(X;\theta) = f(x_1, x_2, ..., x_N; \theta) = {\prod_{i=1}^{N} f(x_i;\theta)} \\\\
  & \mathrm{ln}(L(X;\theta)) = \sum_{i=1}^{N}\mathrm{ln}f(x_i; \theta)
  \end{align}
  $$
  问题即转化为
  $$
  \max_{\theta}arg {1 \over N} {\sum_{i=1}^{N} \mathrm{ln}(f(x_i;\theta))} \\
  \iff \\
  \min_{\theta}arg {1 \over N} {\sum_{i=1}^{N} \mathrm{-ln}(f(x_i;\theta))} \\
  $$



当损失函数 $L(Y, f(X))$ 为对数损失函数时，有 $L(Y, f(Y|X)) = -\mathrm{ln}(f(Y|X))$ ，其中$f$为条件概率模型

故有经验风险最小化等价于极大似然估计

