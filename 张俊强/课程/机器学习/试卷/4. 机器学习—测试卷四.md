<div align="center" style="font-size: 24px; font-weight: bold">机器学习测试卷四</div>

一、基础知识（$2 \times 25$）

1. 泛化能力指的是模型$\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_$。

   泛化误差定义如下
   $$
   \begin{align}
   R_{exp} (\hat{f}) &= E_p[L(Y, \hat{f}(X))] \\\\
   		&= \int_{\mathcal{X} \times \mathcal{Y}} L(y, \hat{f}(x)) \times P(x, y)\ dxdy
   \end{align}
   $$
   其中$\hat{f}(X)$表示上面的 **^** 当前的模型是通过学习得到的。

   当使用平方损失时，泛化误差的表达式为$\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_$。

   随着模型容量的增加，泛化误差$\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_$，偏差$\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_$，方差却组件增大，这是因为$\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_$。

2. 朴素贝叶斯法对条件概率分布作了条件独立性的假设。由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体地，条件独立性假设是：
   $$
   \begin{aligned}
   P(X&=x | Y=c_{k} )=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\ &={\prod_{j=1}^{n}}_{\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
   \end{aligned}
   $$
   
3. 当前样本集合$D$中第$k$类所占的比例为$p_k \quad k = 1, 2, ..., K$，则样本$D$的信息熵定为为
   $$
   Ent(D) =_{\ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_}
   $$
   
4. CART算法(`classification and regression tree algorithm`)的假设决策树是$\_\_\_\_\_\_\_\_\_\_$，其分类树使用$\_\_\_\_\_\_\_\_\_$最小化准则进行特征选择。

5. 样本中有$10$个桃子，其中成熟的有6个，剩下的未成熟，则样本的基尼指数为$\_\_\_\_\_\_\_\_\_$。

   经过测量并统计

   - 以甜度是否大于0.2进行划分$D_1、D_2$两个部分，$D_1$中成熟的有6个，1个未成熟，$D_2$中只有3个未成熟的。
   - 以硬度进行划分为$D_1、D_2$两个部分，$D_1$中成熟的有4个，2个未成熟，$D_2$中有2个成熟的，2个未成熟的。

   则在甜度这一特征的条件下，样本$D$的基尼指数为$\_\_\_\_\_\_\_\_\_\_\_$。

   则在硬度这一特征的条件下，样本$D$的基尼指数为$\_\_\_\_\_\_\_\_\_\_\_$。

   这两个特征中用于划分数据集最优的为$\_\_\_\_\_\_\_\_\_\_\_$。

​    

二、计算题（$2 \times 15$）

1. 已知一个有训练数据集，其正例点是$x_1=(3,3)^T，x_2=(4,3)^T$,负例点是$x_3=(1,1)^T$，求线性可分支持向量机，写出分类超平面和分类决策函数。















2. 给定如下表的训练数据。假设弱分类器由 $x < v$ 或者 $x > v$ 产生，其阈值 $v$ 使该分类器在训练数据集上的误差最低。试着使用 AdaBoost 算法学习一个强分类器。

   | 序号 |  1   |  2   | 3    |  4   |  5   |  6   |  7   |  8   |  9   |  10  |
   | :--: | :--: | :--: | ---- | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
   | $x$  |  0   |  1   | 2    |  3   |  4   |  5   |  6   |  7   |  8   |  9   |
   | $y$  |  1   |  1   | 1    |  -1  |  -1  |  -1  |  1   |  1   |  1   |  -1  |























三、综合题（$2 \times 10$）

1. 假设$f(x)、g_i(x)、h_j(x)$是定义在$R^n$上的连续可微函数。考虑约束最优化问题
   $$
   \begin{align}
   & \min_{x \in R^n} f(x) \\\\
   s.t 
   & \quad g_i(x) \leq 0, \quad i = 1, 2, 3, ... K \\
   & \quad h_j(x) = 0, \quad j = 1, 2, 3, ... L \\
   
   \end{align}
   $$
   称此约束最优化问题为原始最优化问题或原始问题`Prime Problem`

   引入广义拉格朗日函数
   $$
   L(x, \alpha, \beta) = f(x) + \sum_{i=1}^{K} \alpha_ig_i(x) + \sum_{j=1}^{L}\beta_jh_j(x)
   $$
   其中$\alpha_i、\beta_j$为拉格朗日乘子，且$\alpha_i \geq 0$ 。

   令
   $$
   \theta_P(x) = \max_{\alpha, \beta : \alpha_i \geq 0} L(x,\ \alpha,\ \beta)
   $$
   再令
   $$
   \theta_D(\alpha, \beta) = \min_{x} L(x,\ \alpha,\ \beta)
   $$
   若 $x^*$ 是 $\min_x \theta_P(x)$  的解，$\alpha^*、\beta^* $ 是 $\max_{\alpha, \beta : \alpha_i \geq 0} \theta_D(x)$ （对偶问题`Dual Problem`）的解，回答下面的问题

   $(1)$ 说明为什么原始问题`Prime Problem`等价于 $\min_x \theta_P(x)$ 。

   $(2)$ 证明 $\theta_P(x^*) \geq \theta_D(\alpha^*,\ \beta^*)$。

   $(3)$ 若 $(2)$ 中的等号成立，即 $\theta_P(x^*) = \theta_D(\alpha^*,\ \beta^*)$，$x^*、\alpha^* 、\beta^*$ 需要满足的条件有哪些？

   













































